{
 "cells": [
  {
   "cell_type": "code",
   "id": "7a07fd19ef67c22c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T01:38:02.783172400Z",
     "start_time": "2026-01-18T01:37:59.613930500Z"
    }
   },
   "source": [
    "# load movie review dataset(with NER Metadata) - size = 7816\n",
    "import pandas as pd\n",
    "matched_reviews = pd.read_csv(\"clean_dataset.csv\")\n",
    "\n",
    "# print(matched_reviews.tail())\n",
    "# print(matched_reviews.shape)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "576a8f7600880056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:26:25.134307Z",
     "start_time": "2025-11-26T15:26:24.847643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Baseline Classification Models without NER Metadata\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# Prepare data for modeling\n",
    "df = matched_reviews.copy()\n",
    "\n",
    "# Ensure text is string\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20_000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6a4afe24c4746605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:26:27.875209Z",
     "start_time": "2025-11-26T15:26:27.737529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Logistic Regression (Without NER Metadata)====\n",
      "Accuracy: 0.7595907928388747\n",
      "F1 Score: 0.8493589743589743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.28      0.41       459\n",
      "           1       0.76      0.96      0.85      1105\n",
      "\n",
      "    accuracy                           0.76      1564\n",
      "   macro avg       0.75      0.62      0.63      1564\n",
      "weighted avg       0.76      0.76      0.72      1564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model (Without NER Metadata)\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluation\n",
    "pred_lr = log_reg.predict(X_test_vec)\n",
    "\n",
    "print(\"==== Logistic Regression (Without NER Metadata)====\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_lr))\n",
    "print(\"F1 Score:\", f1_score(y_test, pred_lr))\n",
    "print(classification_report(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b097d143e22dde5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:26:30.472427Z",
     "start_time": "2025-11-26T15:26:30.427473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Linear SVM (Without NER Metadata) ====\n",
      "Accuracy: 0.7749360613810742\n",
      "F1 Score: 0.8503401360544217\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.46      0.55       459\n",
      "           1       0.80      0.90      0.85      1105\n",
      "\n",
      "    accuracy                           0.77      1564\n",
      "   macro avg       0.74      0.68      0.70      1564\n",
      "weighted avg       0.76      0.77      0.76      1564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM Model Without NER Metadata\n",
    "svm_clf = LinearSVC()\n",
    "svm_clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluation\n",
    "pred_svm = svm_clf.predict(X_test_vec)\n",
    "\n",
    "print(\"==== Linear SVM (Without NER Metadata) ====\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_svm))\n",
    "print(\"F1 Score:\", f1_score(y_test, pred_svm))\n",
    "print(classification_report(y_test, pred_svm))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Baseline Classification Models with NER Metadata\n",
    "from textblob import TextBlob\n",
    "\n",
    "# A function to compute entity features eg. count the number of time actors/directors were mentioned, entity sentiment, etc\n",
    "def compute_entity_features(row):\n",
    "\n",
    "    # actors/directors are lists, not strings\n",
    "    actors = row.get(\"actors\", [])\n",
    "    directors = row.get(\"directors\", [])\n",
    "\n",
    "    num_actors = len(actors)\n",
    "    num_directors = len(directors)\n",
    "\n",
    "    # review text to lowercase\n",
    "    text = row[\"text\"].lower()\n",
    "\n",
    "    # Count actor mentions\n",
    "    actor_mentions = 0\n",
    "    for a in actors:\n",
    "        actor_mentions += text.count(a.lower())\n",
    "\n",
    "    # Count director mentions\n",
    "    director_mentions = 0\n",
    "    for d in directors:\n",
    "        director_mentions += text.count(d.lower())\n",
    "\n",
    "    # Sentiment toward entity names\n",
    "    entity_tokens = actors + directors\n",
    "    entity_sentiment = 0\n",
    "\n",
    "    if entity_tokens:\n",
    "        combined = \" \".join(entity_tokens)\n",
    "        try:\n",
    "            entity_sentiment = TextBlob(combined).sentiment.polarity\n",
    "        except:\n",
    "            entity_sentiment = 0\n",
    "\n",
    "    return pd.Series({\n",
    "        # \"num_titles\": num_titles,\n",
    "        \"num_actors\": num_actors,\n",
    "        \"num_directors\": num_directors,\n",
    "        \"actor_mentions\": actor_mentions,\n",
    "        \"director_mentions\": director_mentions,\n",
    "        \"entity_sentiment\": entity_sentiment\n",
    "    })\n",
    "\n",
    "\n",
    "# Compute entity features\n",
    "entity_features = matched_reviews.apply(compute_entity_features, axis=1)\n",
    "full_df = pd.concat([matched_reviews, entity_features], axis=1)\n",
    "print(full_df.tail())"
   ],
   "id": "67a46a3dc1ebd8fd"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c021aeaf1246e003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T16:00:30.394489Z",
     "start_time": "2025-11-26T16:00:29.869743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      num_actors  num_directors  actor_mentions  director_mentions  \\\n",
      "1414        33.0            2.0           171.0                0.0   \n",
      "5940        20.0            2.0           216.0                0.0   \n",
      "6152        14.0            2.0            68.0                0.0   \n",
      "57           2.0            2.0             0.0                0.0   \n",
      "4423         2.0            2.0             0.0                0.0   \n",
      "\n",
      "      entity_sentiment  \n",
      "1414               0.0  \n",
      "5940               0.0  \n",
      "6152               0.0  \n",
      "57                 0.0  \n",
      "4423               0.0  \n",
      "(1564,)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split BEFORE vectorization\n",
    "X_text_raw = full_df[\"text\"]\n",
    "X_entity = full_df[[\n",
    "    \"num_actors\", \"num_directors\",\n",
    "    \"actor_mentions\", \"director_mentions\",\n",
    "    \"entity_sentiment\" #  \"num_titles\",\n",
    "]].fillna(0)\n",
    "y = full_df[\"label\"]\n",
    "\n",
    "# Train-test split on raw data\n",
    "X_text_train, X_text_test, X_entity_train, X_entity_test, y_train, y_test = train_test_split(\n",
    "    X_text_raw, X_entity, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(X_entity_test.head())\n",
    "print(X_text_test.shape)\n",
    "# x_with_entity_text = [X_text_test, X_entity_test]\n",
    "# x_with_entity_text.columns\n",
    "\n",
    "# Now fit TF-IDF only on training text\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_train_text = tfidf.fit_transform(X_text_train)  # Learn from train only\n",
    "X_test_text = tfidf.transform(X_text_test)        # Apply to test\n",
    "\n",
    "# Scale entity features (fit on train, transform both)\n",
    "scaler = StandardScaler(with_mean=False)  # Sparse-compatible\n",
    "X_entity_train_scaled = scaler.fit_transform(X_entity_train.values)\n",
    "X_entity_test_scaled = scaler.transform(X_entity_test.values)\n",
    "\n",
    "# Combine features\n",
    "X_train = hstack([X_train_text, X_entity_train_scaled])\n",
    "X_test = hstack([X_test_text, X_entity_test_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b4b9b511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:26:48.648935Z",
     "start_time": "2025-11-26T15:26:48.381945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Entity-Aware Logistic Regression ====\n",
      "Accuracy: 0.7774936061381074\n",
      "F1 Score: 0.8557213930348259\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.40      0.51       459\n",
      "           1       0.79      0.93      0.86      1105\n",
      "\n",
      "    accuracy                           0.78      1564\n",
      "   macro avg       0.75      0.67      0.68      1564\n",
      "weighted avg       0.77      0.78      0.76      1564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with NER features\n",
    "log_clf = LogisticRegression(max_iter=500)\n",
    "log_clf.fit(X_train, y_train)\n",
    "\n",
    "pred_log = log_clf.predict(X_test)\n",
    "\n",
    "print(\"==== Entity-Aware Logistic Regression ====\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_log))\n",
    "print(\"F1 Score:\", f1_score(y_test, pred_log))\n",
    "print(classification_report(y_test, pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e89465c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:26:56.479413Z",
     "start_time": "2025-11-26T15:26:56.385049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Entity-Aware SVM ====\n",
      "Accuracy: 0.7934782608695652\n",
      "F1 Score: 0.8592592592592593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.56      0.61       459\n",
      "           1       0.83      0.89      0.86      1105\n",
      "\n",
      "    accuracy                           0.79      1564\n",
      "   macro avg       0.76      0.72      0.74      1564\n",
      "weighted avg       0.79      0.79      0.79      1564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM with NER features\n",
    "svm_clf = LinearSVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "print(\"==== Entity-Aware SVM ====\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_svm))\n",
    "print(\"F1 Score:\", f1_score(y_test, pred_svm))\n",
    "print(classification_report(y_test, pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "daeac4338fb55900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:27:02.123586Z",
     "start_time": "2025-11-26T15:27:00.926547Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6252/6252 [00:00<00:00, 26413.71 examples/s]\n",
      "Map: 100%|██████████| 1564/1564 [00:00<00:00, 23167.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'num_actors', 'num_directors', 'actor_mentions', 'director_mentions', 'entity_sentiment', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 6252\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "entity_cols = [\"num_actors\", \"num_directors\",\n",
    "               \"actor_mentions\", \"director_mentions\", \"entity_sentiment\"]\n",
    "\n",
    "df = full_df[[\"text\", \"label\"] + entity_cols].dropna()\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# -------------------------\n",
    "# TOKENIZER\n",
    "# -------------------------\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "MAX_LEN = 128   # MUCH FASTER (cut 256 → 128)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    encoded = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=False,       # dynamic padding enabled later\n",
    "    )\n",
    "    # add entity features\n",
    "    for col in entity_cols:\n",
    "        encoded[col] = batch[col]\n",
    "    return encoded\n",
    "\n",
    "# Remove original columns except what we return\n",
    "cols_to_keep = [\"input_ids\", \"attention_mask\", \"label\"] + entity_cols\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[c for c in train_ds.column_names if c not in cols_to_keep]\n",
    ")\n",
    "\n",
    "test_ds = test_ds.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[c for c in test_ds.column_names if c not in cols_to_keep]\n",
    ")\n",
    "\n",
    "\n",
    "train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "test_ds = test_ds.rename_column(\"label\", \"labels\")\n",
    "print(train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "914d855a44c698d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:27:11.273885Z",
     "start_time": "2025-11-26T15:27:11.003148Z"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# NORMALIZE ENTITY FEATURES\n",
    "# -------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[entity_cols] = scaler.fit_transform(df[entity_cols])\n",
    "\n",
    "# -------------------------\n",
    "# DYNAMIC PADDING - Faster GPU and less memory\n",
    "# -------------------------\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# -------------------------\n",
    "# MODEL\n",
    "# -------------------------\n",
    "from transformers import DistilBertModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class DistilBertWithEntities(nn.Module):\n",
    "    def __init__(self, num_labels, entity_dim=6):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(768 + entity_dim, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        # num_titles=None,\n",
    "        num_actors=None,\n",
    "        num_directors=None,\n",
    "        actor_mentions=None,\n",
    "        director_mentions=None,\n",
    "        entity_sentiment=None,\n",
    "    ):\n",
    "        # DistilBERT forward\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "\n",
    "        # Stack entity features\n",
    "        entity_tensors = [\n",
    "            # num_titles.unsqueeze(1).float(),\n",
    "            num_actors.unsqueeze(1).float(),\n",
    "            num_directors.unsqueeze(1).float(),\n",
    "            actor_mentions.unsqueeze(1).float(),\n",
    "            director_mentions.unsqueeze(1).float(),\n",
    "            entity_sentiment.unsqueeze(1).float(),\n",
    "        ]\n",
    "        entity_tensor = torch.cat(entity_tensors, dim=1)\n",
    "\n",
    "        # Combine text + entity features\n",
    "        combined = torch.cat((pooled_output, entity_tensor), dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        logits = self.fc(combined)\n",
    "\n",
    "        # Loss with optional class weights\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            class_counts = torch.bincount(labels)\n",
    "            class_weights = (1.0 / class_counts.float()).to(logits.device)\n",
    "            loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "num_labels = df[\"label\"].nunique()\n",
    "model = DistilBertWithEntities(num_labels, entity_dim=len(entity_cols))\n",
    "\n",
    "# -------------------------\n",
    "# TRAINING ARGS (IMPROVED)\n",
    "# -------------------------\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-entity-improved\",\n",
    "\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "\n",
    "    gradient_accumulation_steps=2,   # effective batch = 32\n",
    "    learning_rate=3e-5,\n",
    "\n",
    "    num_train_epochs=2,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99c8b78471916c69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:40:48.391646Z",
     "start_time": "2025-11-26T15:27:16.470868Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='392' max='392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [392/392 13:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.490900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.313100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=392, training_loss=0.44208768922455455, metrics={'train_runtime': 811.6499, 'train_samples_per_second': 15.406, 'train_steps_per_second': 0.483, 'total_flos': 0.0, 'train_loss': 0.44208768922455455, 'epoch': 2.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# TRAIN\n",
    "# -------------------------\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d5cc2f7ad0695d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:41:49.046539Z",
     "start_time": "2025-11-26T15:40:59.759810Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.34162789583206177,\n",
       " 'eval_runtime': 49.2761,\n",
       " 'eval_samples_per_second': 31.74,\n",
       " 'eval_steps_per_second': 0.994,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Evaluate\n",
    "# -------------------------\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "de848a657a7806ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:42:57.749637Z",
     "start_time": "2025-11-26T15:42:09.989185Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Entity Aware - DistilBERT Evaluation ====\n",
      "Accuracy: 0.8759590792838875\n",
      "F1 Score: 0.877663883797496\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.80       459\n",
      "           1       0.93      0.89      0.91      1105\n",
      "\n",
      "    accuracy                           0.88      1564\n",
      "   macro avg       0.85      0.87      0.85      1564\n",
      "weighted avg       0.88      0.88      0.88      1564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from trainer\n",
    "predictions = trainer.predict(test_ds)\n",
    "\n",
    "# Extract logits and labels\n",
    "preds = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Convert logits to predicted class indices\n",
    "preds = preds.argmax(axis=-1)\n",
    "\n",
    "acc = accuracy_score(labels, preds)\n",
    "f1 = f1_score(labels, preds, average='weighted')  # or 'macro' if you prefer\n",
    "\n",
    "print(\"==== Entity Aware - DistilBERT Evaluation ====\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "39a601622179af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T16:11:54.437452Z",
     "start_time": "2025-11-26T16:11:54.429206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1564, 5)\n"
     ]
    }
   ],
   "source": [
    "# Bias Evaluation - LR and SVM\n",
    "entity_cols = [\"num_actors\", \"num_directors\",\n",
    "               \"actor_mentions\", \"director_mentions\", \"entity_sentiment\"]\n",
    "\n",
    "entity_name_map = {\n",
    "    \"num_actors\": \"Actor Mentions\",\n",
    "    \"num_directors\": \"Director Mentions\",\n",
    "    \"actor_mentions\": \"Specific Actor Mentions\",\n",
    "    \"director_mentions\": \"Specific Director Mentions\",\n",
    "    \"entity_sentiment\": \"Entity Sentiment Score\"\n",
    "}\n",
    "\n",
    "X_entity_test_df = X_entity_test[entity_cols]\n",
    "print(X_entity_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56eed8b3f25bc546",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:02:32.429577Z",
     "start_time": "2025-11-26T15:02:32.249500Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Split into text features and entity features\n",
    "# X_text = vectorizer.transform(X_text_test)   # sparse matrix\n",
    "# X_entities = X_entity_test                     # pandas DataFrame\n",
    "#\n",
    "# # Combine for training\n",
    "# from scipy.sparse import hstack\n",
    "# X_train = hstack([X_text, X_entities.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b752927a8d29f334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T16:53:17.458661Z",
     "start_time": "2025-11-26T16:53:17.449477Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def evaluate_bias(model, X, y, entity_df, entity_cols, entity_name_map, model_name=\"Model\"):\n",
    "    preds = model.predict(X)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    f1 = f1_score(y, preds, average=\"weighted\")\n",
    "    print(f\"\\n==== {model_name} ====\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(classification_report(y, preds))\n",
    "\n",
    "    # Subgroup metrics\n",
    "    for col in entity_cols:\n",
    "        subgroup_idx = entity_df[entity_df[col] > 0].index.to_list()\n",
    "        print(\"sub_idx\", len(subgroup_idx))\n",
    "\n",
    "        if len(subgroup_idx) > 0:\n",
    "            # Use .take() for sparse matrices\n",
    "            subgroup_X = X.tocsr()[subgroup_idx, :]\n",
    "            subgroup_preds = model.predict(subgroup_X)\n",
    "            subgroup_true = y.iloc[subgroup_idx]\n",
    "            print(f\"{entity_name_map[col]} → Acc: {accuracy_score(subgroup_true, subgroup_preds):.3f}, \"\n",
    "                  f\"F1: {f1_score(subgroup_true, subgroup_preds, average='weighted'):.3f}\")\n",
    "\n",
    "# def evaluate_bias(model, X, y, entity_df, entity_name_map, model_name=\"Model\"):\n",
    "#     preds = model.predict(X)\n",
    "#     acc = accuracy_score(y, preds)\n",
    "#     f1 = f1_score(y, preds, average=\"weighted\")\n",
    "#     print(f\"\\n==== {model_name} ====\")\n",
    "#     print(\"Accuracy:\", acc)\n",
    "#     print(\"F1 Score:\", f1)\n",
    "#     print(classification_report(y, preds))\n",
    "#\n",
    "#     # Subgroup metrics\n",
    "#     for col in X_entities:\n",
    "#         subgroup_idx = entity_df[entity_df[col] > 0].index\n",
    "#         if len(subgroup_idx) > 0:\n",
    "#             subgroup_preds = model.predict(X[subgroup_idx])\n",
    "#             subgroup_true = y.iloc[subgroup_idx]\n",
    "#             print(f\"{entity_name_map[col]} → Acc: {accuracy_score(subgroup_true, subgroup_preds):.3f}, \"\n",
    "#                   f\"F1: {f1_score(subgroup_true, subgroup_preds, average='weighted'):.3f}\")\n",
    "\n",
    "    # for col in entity_cols:\n",
    "    #     subgroup = X[X[col] > 0]\n",
    "    #     if len(subgroup) > 0:\n",
    "    #         subgroup_preds = model.predict(subgroup)\n",
    "    #         subgroup_acc = accuracy_score(y.loc[subgroup.index], subgroup_preds)\n",
    "    #         subgroup_f1 = f1_score(y.loc[subgroup.index], subgroup_preds, average=\"weighted\")\n",
    "    #         print(f\"{entity_name_map[col]} → Acc: {subgroup_acc:.3f}, F1: {subgroup_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7632fa526a39f2fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T16:53:22.499008Z",
     "start_time": "2025-11-26T16:53:22.348760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Entity-Aware Logistic Regression - Bias Evaluation ====\n",
      "Accuracy: 0.7774936061381074\n",
      "F1 Score: 0.7554237556448992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.40      0.51       459\n",
      "           1       0.79      0.93      0.86      1105\n",
      "\n",
      "    accuracy                           0.78      1564\n",
      "   macro avg       0.75      0.67      0.68      1564\n",
      "weighted avg       0.77      0.78      0.76      1564\n",
      "\n",
      "sub_idx 1564\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index (7813) out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[141]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mevaluate_bias\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_clf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_entity_test_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mentity_cols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mentity_name_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mEntity-Aware Logistic Regression - Bias Evaluation\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m evaluate_bias(svm_clf, X_test, y_test, X_entity_test_df, entity_cols, entity_name_map, \u001B[33m\"\u001B[39m\u001B[33mEntity-Aware SVM - Bias Evaluation\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[140]\u001B[39m\u001B[32m, line 19\u001B[39m, in \u001B[36mevaluate_bias\u001B[39m\u001B[34m(model, X, y, entity_df, entity_cols, entity_name_map, model_name)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33msub_idx\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(subgroup_idx))\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(subgroup_idx) > \u001B[32m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m subgroup_idx != \u001B[38;5;28mrange\u001B[39m(\u001B[32m0\u001B[39m, \u001B[32m7813\u001B[39m):\n\u001B[32m     18\u001B[39m     \u001B[38;5;66;03m# Use .take() for sparse matrices\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m     subgroup_X = \u001B[43mX\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtocsr\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43msubgroup_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     20\u001B[39m     subgroup_preds = model.predict(subgroup_X)\n\u001B[32m     21\u001B[39m     subgroup_true = y.iloc[subgroup_idx]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\scipy\\sparse\\_index.py:30\u001B[39m, in \u001B[36mIndexMixin.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m     index, new_shape = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_validate_indices\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m     \u001B[38;5;66;03m# 1D array\u001B[39;00m\n\u001B[32m     33\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(index) == \u001B[32m1\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\scipy\\sparse\\_index.py:288\u001B[39m, in \u001B[36mIndexMixin._validate_indices\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m    286\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# dense array\u001B[39;00m\n\u001B[32m    287\u001B[39m     N = \u001B[38;5;28mself\u001B[39m._shape[index_ndim]\n\u001B[32m--> \u001B[39m\u001B[32m288\u001B[39m     idx = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_asindices\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mN\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    289\u001B[39m     index.append(idx)\n\u001B[32m    290\u001B[39m     array_indices.append(index_ndim)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\scipy\\sparse\\_index.py:332\u001B[39m, in \u001B[36mIndexMixin._asindices\u001B[39m\u001B[34m(self, idx, length)\u001B[39m\n\u001B[32m    330\u001B[39m max_indx = x.max()\n\u001B[32m    331\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m max_indx >= length:\n\u001B[32m--> \u001B[39m\u001B[32m332\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mindex (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_indx\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) out of range\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    334\u001B[39m min_indx = x.min()\n\u001B[32m    335\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m min_indx < \u001B[32m0\u001B[39m:\n",
      "\u001B[31mIndexError\u001B[39m: index (7813) out of range"
     ]
    }
   ],
   "source": [
    "evaluate_bias(log_clf, X_test, y_test, X_entity_test_df, entity_cols, entity_name_map, \"Entity-Aware Logistic Regression - Bias Evaluation\")\n",
    "evaluate_bias(svm_clf, X_test, y_test, X_entity_test_df, entity_cols, entity_name_map, \"Entity-Aware SVM - Bias Evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
