{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-21T01:11:51.992698Z",
     "start_time": "2025-11-21T01:11:27.396059Z"
    }
   },
   "source": [
    "# Load the dataset\n",
    "from  datasets import load_dataset\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")  \n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "train_data[0]\n",
    "# test_data[0]\n",
    "# print(test_data[0])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\PycharmProjects\\Nlp-final-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T15:25:16.208413Z",
     "start_time": "2025-11-20T15:25:16.196627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def get_movie_title():\n",
    "#\n",
    "#     sample_titles = []\n",
    "#     with open(\"imdb_top_movies.txt\", \"r\") as f:\n",
    "#         for l in f.readlines():\n",
    "#             line = l.strip()\n",
    "#             sample_titles.append(line)\n",
    "#\n",
    "#         print(\"movie titles\",sample_titles)\n",
    "#     return sample_titles\n"
   ],
   "id": "a19b7590e3a0984b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:32:42.176380Z",
     "start_time": "2025-11-21T01:15:29.387162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "import spacy\n",
    "\n",
    "# ------------------------------\n",
    "# Load API key and spaCy model\n",
    "# ------------------------------\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # NER model\n",
    "\n",
    "# ------------------------------\n",
    "# Normalize text function\n",
    "# ------------------------------\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize(\"NFKC\", text).lower()\n",
    "\n",
    "# ------------------------------\n",
    "# Movie titles\n",
    "# ------------------------------\n",
    "\n",
    "#\n",
    "sample_titles = [\n",
    "   \"The Shawshank Redemption\", \"The Godfather\", \"The Dark Knight\", \"Pulp Fiction\", \"Forrest Gump\",\n",
    "    \"Fight Club\", \"Inception\", \"The Matrix\", \"Goodfellas\", \"The Lord of the Rings: The Return of the King\",\n",
    "    \"Interstellar\", \"Parasite\", \"The Silence of the Lambs\", \"Saving Private Ryan\", \"Schindler’s List\",\n",
    "    \"Gladiator\", \"Titanic\", \"The Green Mile\", \"The Departed\", \"Django Unchained\",\n",
    "    \"The Prestige\", \"Whiplash\", \"The Lion King\", \"Toy Story\", \"Avengers: Endgame\",\n",
    "    \"Avengers: Infinity War\", \"Iron Man\", \"Black Panther\", \"Joker\", \"The Social Network\",\n",
    "    \"The Wolf of Wall Street\", \"La La Land\", \"Mad Max: Fury Road\", \"The Revenant\", \"Get Out\",\n",
    "    \"Oppenheimer\", \"Barbie\", \"Dune\", \"The Batman\", \"Spider-Man: No Way Home\",\n",
    "    \"Everything Everywhere All at Once\", \"The Irishman\", \"12 Years a Slave\", \"Moonlight\", \"Spotlight\",\n",
    "    \"Birdman\", \"Arrival\", \"Blade Runner 2049\", \"No Country for Old Men\", \"The Big Short\",\n",
    "    \"The Hateful Eight\", \"Once Upon a Time in Hollywood\", \"There Will Be Blood\",\n",
    "    \"The Curious Case of Benjamin Button\", \"The Shape of Water\", \"The Theory of Everything\",\n",
    "    \"Bohemian Rhapsody\", \"Rocketman\", \"A Star Is Born\", \"The Imitation Game\", \"The King's Speech\",\n",
    "    \"Slumdog Millionaire\", \"Life of Pi\", \"Gravity\", \"Cast Away\", \"The Truman Show\",\n",
    "    \"Eternal Sunshine of the Spotless Mind\", \"Requiem for a Dream\", \"American Beauty\", \"The Sixth Sense\",\n",
    "    \"Se7en\", \"The Usual Suspects\", \"Memento\", \"Oldboy\", \"Pan’s Labyrinth\",\n",
    "    \"Amélie\", \"The Pianist\", \"The Lives of Others\", \"City of God\", \"Crouching Tiger, Hidden Dragon\",\n",
    "    \"Spirited Away\", \"Howl’s Moving Castle\", \"Princess Mononoke\", \"My Neighbor Totoro\", \"WALL·E\",\n",
    "    \"Up\", \"Inside Out\", \"Coco\", \"Soul\", \"Minari\", \"The Banshees of Inisherin\",\n",
    "    \"Casino Royale\", \"South Park: Bigger, Longer & Uncut\", \"A Fistful of Dollars\", \"Rosemary's Baby\",\n",
    "    \"The Incredibles\", \"Black Swan\", \"Deadpool\", \"The Breakfast Club\", \"The Untouchables\",\n",
    "    \"Shaun of the Dead\", \"True Romance\", \"Harry Potter and the Prisoner of Azkaban\", \"Hot Fuzz\",\n",
    "    \"In Bruges\", \"Boyhood\", \"Straight Outta Compton\", \"Drive\", \"Moneyball\", \"Brazil\", \"Chronicle\",\n",
    "    \"Still Alice\", \"Triangle\", \"The Endless\", \"The Man from Earth\",\n",
    "    \"The Secret in Their Eyes\", \"The Fall\", \"The Hunt\", \"Incendies\", \"The Intouchables\",\n",
    "    \"Prisoners\", \"Enemy\", \"Locke\", \"The Lobster\", \"Under the Skin\",\n",
    "    \"Ex Machina\", \"Annihilation\", \"The Florida Project\", \"Room\", \"Brooklyn\",\n",
    "    \"Carol\", \"The Farewell\", \"Portrait of a Lady on Fire\", \"The Handmaiden\", \"Shoplifters\",\n",
    "    \"A Separation\", \"Toni Erdmann\", \"Cold War\", \"Wild Tales\", \"The Square\"\n",
    "]\n",
    "\n",
    "# normalize titles for regex patterns\n",
    "sample_titles_norm = [normalize_text(t) for t in sample_titles]\n",
    "\n",
    "title_patterns = {\n",
    "    title: re.compile(r\"(?<!\\w)\" + re.escape(title) + r\"(?!\\w)\", re.IGNORECASE)\n",
    "    for title in sample_titles_norm\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Detect titles function\n",
    "# ------------------------------\n",
    "def detect_titles_regex(text, patterns):\n",
    "    text_norm = normalize_text(text)\n",
    "    detected = []\n",
    "    for title, pattern in patterns.items():\n",
    "        if pattern.search(text_norm):\n",
    "            detected.append(title)\n",
    "    return detected\n",
    "\n",
    "# ------------------------------\n",
    "# TMDb metadata caching\n",
    "# ------------------------------\n",
    "metadata_cache = {}\n",
    "\n",
    "def get_movie_metadata(title):\n",
    "    title_key = title.strip().lower()\n",
    "    if title_key in metadata_cache:\n",
    "        return metadata_cache[title_key]\n",
    "\n",
    "    query_title = title.strip().title()  # proper case for API\n",
    "    search_url = f\"{BASE_URL}/search/movie?api_key={API_KEY}&query={query_title}\"\n",
    "    try:\n",
    "        search_response = requests.get(search_url).json()\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "    if not search_response.get(\"results\"):\n",
    "        return None\n",
    "\n",
    "    movie_id = search_response[\"results\"][0][\"id\"]\n",
    "\n",
    "    credits_url = f\"{BASE_URL}/movie/{movie_id}/credits?api_key={API_KEY}\"\n",
    "    try:\n",
    "        credits_response = requests.get(credits_url).json()\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "    actors = [member[\"name\"] for member in credits_response.get(\"cast\", [])[:7]]\n",
    "    directors = [member[\"name\"] for member in credits_response.get(\"crew\", []) if member[\"job\"] == \"Director\"]\n",
    "\n",
    "    result = {\"actors\": actors, \"directors\": directors}\n",
    "    metadata_cache[title_key] = result\n",
    "    sleep(0.25)  # rate limit\n",
    "    return result\n",
    "\n",
    "# ------------------------------\n",
    "# Enrich with metadata\n",
    "# ------------------------------\n",
    "def enrich_with_metadata(row):\n",
    "    titles = row.get(\"detected_titles\", [])\n",
    "    if not titles:\n",
    "        row[\"actors\"] = None\n",
    "        row[\"directors\"] = None\n",
    "        row[\"ner_entities\"] = []\n",
    "        return row\n",
    "\n",
    "    metadata = get_movie_metadata(titles[0])\n",
    "    row[\"actors\"] = metadata.get(\"actors\") if metadata else []\n",
    "    row[\"directors\"] = metadata.get(\"directors\") if metadata else []\n",
    "\n",
    "    # ------------------------------\n",
    "    # Use spaCy NER to detect persons in review\n",
    "    # ------------------------------\n",
    "    doc = nlp(row[\"text\"])\n",
    "    persons_in_review = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "    # ------------------------------\n",
    "    # Link detected persons to movie metadata\n",
    "    # ------------------------------\n",
    "    relevant_entities = []\n",
    "    for person in persons_in_review:\n",
    "        if person in row[\"actors\"] or person in row[\"directors\"]:\n",
    "            relevant_entities.append(person)\n",
    "\n",
    "    row[\"ner_entities\"] = relevant_entities\n",
    "    return row\n",
    "\n",
    "# ------------------------------\n",
    "# Load dataset\n",
    "# ------------------------------\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "# Detect titles\n",
    "train_df[\"detected_titles\"] = train_df[\"text\"].apply(lambda x: detect_titles_regex(x, title_patterns))\n",
    "\n",
    "# Filter rows with at least one detected title\n",
    "matched_reviews = train_df[train_df[\"detected_titles\"].map(len) > 0].copy()\n",
    "\n",
    "# Initialize columns\n",
    "matched_reviews[\"actors\"] = None\n",
    "matched_reviews[\"directors\"] = None\n",
    "matched_reviews[\"ner_entities\"] = None\n",
    "\n",
    "# Enrich with metadata and NER\n",
    "matched_reviews = matched_reviews.apply(enrich_with_metadata, axis=1)\n",
    "\n",
    "# ------------------------------\n",
    "# Save final DataFrame - for development purposes\n",
    "# ------------------------------\n",
    "matched_reviews.to_csv(\"matched_reviews_with_metadata_ner.csv\", index=False)\n",
    "print(\"Pipeline complete. Saved matched reviews with NER metadata.\")\n",
    "\n",
    "#Took about 38 minutes to run the full pipeline on the training set.\n"
   ],
   "id": "290f66d74ace2bf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline complete. Saved matched reviews with NER metadata.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T15:44:40.345719Z",
     "start_time": "2025-11-20T15:44:40.310006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered = matched_reviews[matched_reviews[\"detected_titles\"] != \"up\"][10:]\n",
    "filtered.head(10)\n",
    "\n",
    "\n"
   ],
   "id": "2404fef39e00aabc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 text  label detected_titles  \\\n",
       "28  Some films that you pick up for a pound turn o...      0            [up]   \n",
       "29  I received this movie as a gift, I knew from t...      0            [up]   \n",
       "30  I have not seen many low budget films i must a...      0            [up]   \n",
       "31  ..Oh wait, I can! This movie is not for the ty...      0            [up]   \n",
       "33  THE ZOMBIE CHRONICLES <br /><br />Aspect ratio...      0            [up]   \n",
       "42  WARNING: This review contains SPOILERS. Do not...      0            [up]   \n",
       "44  Jill Dunne (played by Mitzi Kapture), is an at...      0            [up]   \n",
       "45  This movie sucked. It really was a waste of my...      0            [up]   \n",
       "46  Lifetime did it again. Can we say stupid? I co...      0            [up]   \n",
       "49  The annoying mouse and lullaby really got to m...      0            [up]   \n",
       "\n",
       "                                               actors      directors  \\\n",
       "28  [Ed Asner, Christopher Plummer, Jordan Nagai, ...  [Pete Docter]   \n",
       "29  [Ed Asner, Christopher Plummer, Jordan Nagai, ...  [Pete Docter]   \n",
       "30  [Ed Asner, Christopher Plummer, Jordan Nagai, ...  [Pete Docter]   \n",
       "31  [Ed Asner, Christopher Plummer, Jordan Nagai, ...  [Pete Docter]   \n",
       "33  [Ed Asner, Christopher Plummer, Jordan Nagai, ...  [Pete Docter]   \n",
       "42  [Ed Asner, Christopher Plummer, Jordan Nagai, ...  [Pete Docter]   \n",
       "44  [Ed Asner, Christopher Plummer, Jordan Nagai, ...  [Pete Docter]   \n",
       "45  [Ed Asner, Christopher Plummer, Jordan Nagai, ...  [Pete Docter]   \n",
       "46  [Ed Asner, Christopher Plummer, Jordan Nagai, ...  [Pete Docter]   \n",
       "49  [Ed Asner, Christopher Plummer, Jordan Nagai, ...  [Pete Docter]   \n",
       "\n",
       "   ner_entities  \n",
       "28           []  \n",
       "29           []  \n",
       "30           []  \n",
       "31           []  \n",
       "33           []  \n",
       "42           []  \n",
       "44           []  \n",
       "45           []  \n",
       "46           []  \n",
       "49           []  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>detected_titles</th>\n",
       "      <th>actors</th>\n",
       "      <th>directors</th>\n",
       "      <th>ner_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Some films that you pick up for a pound turn o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[up]</td>\n",
       "      <td>[Ed Asner, Christopher Plummer, Jordan Nagai, ...</td>\n",
       "      <td>[Pete Docter]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I received this movie as a gift, I knew from t...</td>\n",
       "      <td>0</td>\n",
       "      <td>[up]</td>\n",
       "      <td>[Ed Asner, Christopher Plummer, Jordan Nagai, ...</td>\n",
       "      <td>[Pete Docter]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I have not seen many low budget films i must a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[up]</td>\n",
       "      <td>[Ed Asner, Christopher Plummer, Jordan Nagai, ...</td>\n",
       "      <td>[Pete Docter]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>..Oh wait, I can! This movie is not for the ty...</td>\n",
       "      <td>0</td>\n",
       "      <td>[up]</td>\n",
       "      <td>[Ed Asner, Christopher Plummer, Jordan Nagai, ...</td>\n",
       "      <td>[Pete Docter]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>THE ZOMBIE CHRONICLES &lt;br /&gt;&lt;br /&gt;Aspect ratio...</td>\n",
       "      <td>0</td>\n",
       "      <td>[up]</td>\n",
       "      <td>[Ed Asner, Christopher Plummer, Jordan Nagai, ...</td>\n",
       "      <td>[Pete Docter]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>WARNING: This review contains SPOILERS. Do not...</td>\n",
       "      <td>0</td>\n",
       "      <td>[up]</td>\n",
       "      <td>[Ed Asner, Christopher Plummer, Jordan Nagai, ...</td>\n",
       "      <td>[Pete Docter]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Jill Dunne (played by Mitzi Kapture), is an at...</td>\n",
       "      <td>0</td>\n",
       "      <td>[up]</td>\n",
       "      <td>[Ed Asner, Christopher Plummer, Jordan Nagai, ...</td>\n",
       "      <td>[Pete Docter]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>This movie sucked. It really was a waste of my...</td>\n",
       "      <td>0</td>\n",
       "      <td>[up]</td>\n",
       "      <td>[Ed Asner, Christopher Plummer, Jordan Nagai, ...</td>\n",
       "      <td>[Pete Docter]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Lifetime did it again. Can we say stupid? I co...</td>\n",
       "      <td>0</td>\n",
       "      <td>[up]</td>\n",
       "      <td>[Ed Asner, Christopher Plummer, Jordan Nagai, ...</td>\n",
       "      <td>[Pete Docter]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>The annoying mouse and lullaby really got to m...</td>\n",
       "      <td>0</td>\n",
       "      <td>[up]</td>\n",
       "      <td>[Ed Asner, Christopher Plummer, Jordan Nagai, ...</td>\n",
       "      <td>[Pete Docter]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "576a8f7600880056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:35:19.563055Z",
     "start_time": "2025-11-21T01:34:58.729165Z"
    }
   },
   "source": [
    "# Baseline Classification Models without NER Metadata\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# Prepare data for modeling\n",
    "df = matched_reviews.copy()\n",
    "\n",
    "# Ensure text is string\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = df[\"label\"]     # 0 = negative, 1 = positive\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20_000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "6a4afe24c4746605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:36:09.910446Z",
     "start_time": "2025-11-21T01:36:09.670385Z"
    }
   },
   "source": [
    "# Logistic Regression Model\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluation\n",
    "pred_lr = log_reg.predict(X_test_vec)\n",
    "\n",
    "print(\"==== Logistic Regression ====\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_lr))\n",
    "print(\"F1 Score:\", f1_score(y_test, pred_lr))\n",
    "print(classification_report(y_test, pred_lr))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Logistic Regression ====\n",
      "Accuracy: 0.8694191070571291\n",
      "F1 Score: 0.8698564593301435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87      1064\n",
      "           1       0.85      0.89      0.87      1019\n",
      "\n",
      "    accuracy                           0.87      2083\n",
      "   macro avg       0.87      0.87      0.87      2083\n",
      "weighted avg       0.87      0.87      0.87      2083\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "b097d143e22dde5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:36:20.208012Z",
     "start_time": "2025-11-21T01:36:20.037364Z"
    }
   },
   "source": [
    "# SVM Model\n",
    "svm_clf = LinearSVC()\n",
    "svm_clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluation\n",
    "pred_svm = svm_clf.predict(X_test_vec)\n",
    "\n",
    "print(\"==== Linear SVM ====\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_svm))\n",
    "print(\"F1 Score:\", f1_score(y_test, pred_svm))\n",
    "print(classification_report(y_test, pred_svm))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Linear SVM ====\n",
      "Accuracy: 0.8684589534325492\n",
      "F1 Score: 0.8676328502415459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87      1064\n",
      "           1       0.85      0.88      0.87      1019\n",
      "\n",
      "    accuracy                           0.87      2083\n",
      "   macro avg       0.87      0.87      0.87      2083\n",
      "weighted avg       0.87      0.87      0.87      2083\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "10057214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:36:31.463308Z",
     "start_time": "2025-11-21T01:36:25.540244Z"
    }
   },
   "source": [
    "# Baseline Classification Models with NER Metadata\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "\n",
    "def compute_entity_features(row):\n",
    "    titles = row[\"detected_titles\"]\n",
    "\n",
    "    # actors/directors are lists, not strings\n",
    "    actors = row.get(\"actors\", [])\n",
    "    directors = row.get(\"directors\", [])\n",
    "\n",
    "    num_titles = len(titles)\n",
    "    num_actors = len(actors)\n",
    "    num_directors = len(directors)\n",
    "\n",
    "    text = row[\"text\"].lower()\n",
    "\n",
    "    # Count actor mentions\n",
    "    actor_mentions = 0\n",
    "    for a in actors:\n",
    "        actor_mentions += text.count(a.lower())\n",
    "\n",
    "    # Count director mentions\n",
    "    director_mentions = 0\n",
    "    for d in directors:\n",
    "        director_mentions += text.count(d.lower())\n",
    "\n",
    "    # Sentiment toward entity names\n",
    "    entity_tokens = actors + directors\n",
    "    entity_sentiment = 0\n",
    "\n",
    "    if entity_tokens:\n",
    "        combined = \" \".join(entity_tokens)\n",
    "        try:\n",
    "            entity_sentiment = TextBlob(combined).sentiment.polarity\n",
    "        except:\n",
    "            entity_sentiment = 0\n",
    "\n",
    "    return pd.Series({\n",
    "        \"num_titles\": num_titles,\n",
    "        \"num_actors\": num_actors,\n",
    "        \"num_directors\": num_directors,\n",
    "        \"actor_mentions\": actor_mentions,\n",
    "        \"director_mentions\": director_mentions,\n",
    "        \"entity_sentiment\": entity_sentiment\n",
    "    })\n",
    "\n",
    "\n",
    "# Compute entity features\n",
    "entity_features = matched_reviews.apply(compute_entity_features, axis=1)\n",
    "full_df = pd.concat([matched_reviews, entity_features], axis=1)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "3fd9c56a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:34:10.642823Z",
     "start_time": "2025-11-20T16:34:00.352302Z"
    }
   },
   "source": [
    "# from scipy.sparse import hstack\n",
    "#\n",
    "# # TF-IDF Vectorization on review text with NER features\n",
    "# tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "# X_text = tfidf.fit_transform(full_df[\"text\"])\n",
    "#\n",
    "# X_train_text = tfidf.fit_transform(X_train_text_raw)\n",
    "# X_test_text = tfidf.transform(X_test_text_raw)\n",
    "#\n",
    "# # Combine text features with NER features\n",
    "# X_entity = full_df[[\n",
    "#     \"num_titles\", \"num_actors\", \"num_directors\",\n",
    "#     \"actor_mentions\", \"director_mentions\",\n",
    "#     \"entity_sentiment\"\n",
    "# ]].fillna(0).values\n",
    "#\n",
    "# X = hstack([X_text, X_entity])\n",
    "# y = full_df[\"label\"]\n",
    "#\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:36:46.209522Z",
     "start_time": "2025-11-21T01:36:36.877906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split BEFORE vectorization\n",
    "X_text_raw = full_df[\"text\"]\n",
    "X_entity = full_df[[\n",
    "    \"num_titles\", \"num_actors\", \"num_directors\",\n",
    "    \"actor_mentions\", \"director_mentions\",\n",
    "    \"entity_sentiment\"\n",
    "]].fillna(0).values\n",
    "y = full_df[\"label\"]\n",
    "\n",
    "# Train-test split on raw data\n",
    "X_text_train, X_text_test, X_entity_train, X_entity_test, y_train, y_test = train_test_split(\n",
    "    X_text_raw, X_entity, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Now fit TF-IDF only on training text\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_train_text = tfidf.fit_transform(X_text_train)  # Learn from train only\n",
    "X_test_text = tfidf.transform(X_text_test)        # Apply to test\n",
    "\n",
    "# Scale entity features (fit on train, transform both)\n",
    "scaler = StandardScaler(with_mean=False)  # Sparse-compatible\n",
    "X_entity_train_scaled = scaler.fit_transform(X_entity_train)\n",
    "X_entity_test_scaled = scaler.transform(X_entity_test)\n",
    "\n",
    "# Combine features\n",
    "X_train = hstack([X_train_text, X_entity_train])\n",
    "X_test = hstack([X_test_text, X_entity_test])"
   ],
   "id": "c021aeaf1246e003",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "b4b9b511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:36:57.213197Z",
     "start_time": "2025-11-21T01:36:51.641326Z"
    }
   },
   "source": [
    "# Logistic Regression with NER features\n",
    "log_clf = LogisticRegression(max_iter=500)\n",
    "log_clf.fit(X_train, y_train)\n",
    "\n",
    "pred_log = log_clf.predict(X_test)\n",
    "\n",
    "print(\"==== Entity-Aware Logistic Regression ====\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_log))\n",
    "print(\"F1 Score:\", f1_score(y_test, pred_log))\n",
    "print(classification_report(y_test, pred_log))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Entity-Aware Logistic Regression ====\n",
      "Accuracy: 0.8660585693710994\n",
      "F1 Score: 0.8656716417910447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87      1064\n",
      "           1       0.85      0.88      0.87      1019\n",
      "\n",
      "    accuracy                           0.87      2083\n",
      "   macro avg       0.87      0.87      0.87      2083\n",
      "weighted avg       0.87      0.87      0.87      2083\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "8e89465c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T01:37:07.455663Z",
     "start_time": "2025-11-21T01:37:06.860958Z"
    }
   },
   "source": [
    "# SVM with NER features\n",
    "svm_clf = LinearSVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "print(\"==== Entity-Aware SVM ====\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_svm))\n",
    "print(\"F1 Score:\", f1_score(y_test, pred_svm))\n",
    "print(classification_report(y_test, pred_svm))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Entity-Aware SVM ====\n",
      "Accuracy: 0.8636581853096496\n",
      "F1 Score: 0.8633301251203079\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86      1064\n",
      "           1       0.85      0.88      0.86      1019\n",
      "\n",
      "    accuracy                           0.86      2083\n",
      "   macro avg       0.86      0.86      0.86      2083\n",
      "weighted avg       0.86      0.86      0.86      2083\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-21T01:43:37.867036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DistilBERT Model for Sentiment Classification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Custom Dataset Class\n",
    "# ------------------------------\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ------------------------------\n",
    "# Prepare data\n",
    "# ------------------------------\n",
    "# Use the same train-test split as before\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    full_df[\"text\"], full_df[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=full_df[\"label\"]\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ReviewDataset(X_train_text, y_train, tokenizer)\n",
    "test_dataset = ReviewDataset(X_test_text, y_test, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ------------------------------\n",
    "# Initialize Model\n",
    "# ------------------------------\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2  # binary classification\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# ------------------------------\n",
    "# Training Function\n",
    "# ------------------------------\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluation Function\n",
    "# ------------------------------\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(true_labels)\n",
    "\n",
    "# ------------------------------\n",
    "# Train the Model\n",
    "# ------------------------------\n",
    "num_epochs = 3\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    print(f\"Average training loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate on test set after each epoch\n",
    "    predictions, true_labels = evaluate(model, test_loader, device)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Final Evaluation\n",
    "# ------------------------------\n",
    "print(\"\\n==== DistilBERT Final Results ====\")\n",
    "predictions, true_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predictions))\n",
    "print(\"F1 Score:\", f1_score(true_labels, predictions))\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# ------------------------------\n",
    "# Save model (optional)\n",
    "# ------------------------------\n",
    "model.save_pretrained(\"./distilbert_sentiment_model\")\n",
    "tokenizer.save_pretrained(\"./distilbert_sentiment_model\")\n",
    "print(\"\\nModel saved to ./distilbert_sentiment_model\")"
   ],
   "id": "cbb2a9524f7197fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/521 [04:07<18:53:18, 131.02s/it]"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
