# Project Proposal Sentiment Analysis on Movie Reviews##### Group Members - Abdoulie Jallow - Charles Adomako Basoah - Alieu Samateh### OVERVIEWMovie reviews often reflect not only the audience’s reaction to the film itself but also implicit biases toward the actors or directors involved. For instance, a review of a mediocre film starring a beloved actor may skew positively, while a strong film led by a controversial figure may receive harsher sentiment. This project explores whether sentiment in movie reviews is disproportionately influenced by the presence of specific actors or directors, rather than the film’s content alone. Understanding this bias is crucial for improving fairness and interpretability in sentiment analysis systems, especially in recommendation engines and reputation modeling..### TASKThis is a binary classification and bias detection task. The input is a movie review containing named entities (actors or directors), and the output is a sentiment label (positive or negative), along with an attribution score indicating whether the sentiment is more influenced by the named entity or the movie content. The goal is to detect and quantify sentiment bias toward specific individuals.###  DATASETWe will use the IMDb Large Movie Review Dataset (developed by the Stanford Natural Language Processing Group) for sentiment-labeled reviews, combined with metadata from TMDb (The Movie Database) to extract cast and crew information. This dataset consists of 25,000 training instances and 25,000 testing instances, all extracted from movie reviews. Each sentence is labeled as positive or negative to support binary classification tasks. Named entity recognition (NER) will be used to identify mentions of actors and directors in the review text. We will also explore using pre-trained entity linking models to match mentions to known individuals. The dataset is publicly available on the Hugging Face Datasets Hub at https://huggingface.co/datasets/stanfordnlp/imdb### PROPOSED APPROACHWe propose a hybrid approach that combines sentiment classification with bias attribution. First, we will apply named entity recognition (NER) to extract mentions of actors and directors from movie reviews. Next, we will fine-tune DistilBERT to perform sentiment classification on the review text. To interpret the model’s predictions, we will use attention weights or SHAP values to identify which tokens (particularly named entities) contribute most to the sentiment outcome. Finally, we will quantify bias by comparing sentiment scores of reviews that mention specific individuals against those that do not, while controlling for confounding factors such as movie rating and genre. This approach will enable us to not only classify sentiment but also assess whether named entities disproportionately influence sentiment predictions.### BASELINES AND BENCHMARKSWe will compare our approach against a most-frequent class baseline and a sentiment classifier that ignores Named Entity Recognition (NER). We will use logistic regression and linear SVM as classical baselines for comparison and evaluate their results against the DistilBERT transformer-based model. This will help determine whether incorporating entity-level attribution improves bias detection.### EVALUATION METRICWe will evaluate using macro-average F1 for sentiment classification to account for class imbalance. For bias attribution, we will use token-level attribution scores and entity-level sentiment skew, which is defined as the average sentiment deviation when an actor/director is mentioned versus not mentioned. These metrics will help quantify both classification performance and bias magnitude.